{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'> Text Generation with TensorFlow</div></b>","metadata":{}},{"cell_type":"markdown","source":"![](https://img.freepik.com/free-photo/mother-daughter-business-workers-smiling-confident-working-office_839833-10625.jpg?w=1380&t=st=1694628913~exp=1694629513~hmac=7816cdfa99022330d58c52023e337ffe53aef04e0a9b19f5d21778b876c82e9e)","metadata":{}},{"cell_type":"markdown","source":"In this notebook, we'll walk you through how to generate text using a character RNN model. Here are the topics we'll cover:\n- Imports the required libraries\n- Downloads the Shakespeare dataset\n- Preprocesses the text data\n- Defines a model architecture\n- Compiles the model\n- Trains the model\n- Generates text using the trained model","metadata":{}},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>1. Data Loading</div></b>","metadata":{}},{"cell_type":"markdown","source":"In this section, we begin by importing the TensorFlow library and proceed to download a dataset containing Shakespearean text from a remote URL. The downloaded text is stored in a variable called text, and we display the first 100 characters of the text for initial exploration.","metadata":{}},{"cell_type":"code","source":"# Let's import TensorFlow library:\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:05:58.059954Z","iopub.execute_input":"2023-09-13T17:05:58.060541Z","iopub.status.idle":"2023-09-13T17:06:06.730186Z","shell.execute_reply.started":"2023-09-13T17:05:58.060493Z","shell.execute_reply":"2023-09-13T17:06:06.729165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's define the URL for the Shakespeare dataset:\npath = \"https://raw.githubusercontent.com/TirendazAcademy/Deep-Learning-with-TensorFlow/main/Data/tinyshakespeare.txt\"\n\n# Let's download the dataset and save it as 'shakespeare.txt':\nfilepath = tf.keras.utils.get_file(\"shakespeare.txt\", path)\n\n# Let's open and read the downloaded text file:\nwith open(filepath) as f:\n    text = f.read()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:06:06.732303Z","iopub.execute_input":"2023-09-13T17:06:06.732998Z","iopub.status.idle":"2023-09-13T17:06:06.991353Z","shell.execute_reply.started":"2023-09-13T17:06:06.732963Z","shell.execute_reply":"2023-09-13T17:06:06.987712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's display the first 100 characters of the text:\nprint(text[:100])","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:06:06.995031Z","iopub.execute_input":"2023-09-13T17:06:06.995487Z","iopub.status.idle":"2023-09-13T17:06:07.003682Z","shell.execute_reply.started":"2023-09-13T17:06:06.995444Z","shell.execute_reply":"2023-09-13T17:06:07.002560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's examine characters:\n\"\".join(sorted(set(text.lower())))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:06:07.007036Z","iopub.execute_input":"2023-09-13T17:06:07.008419Z","iopub.status.idle":"2023-09-13T17:06:07.054600Z","shell.execute_reply.started":"2023-09-13T17:06:07.008367Z","shell.execute_reply":"2023-09-13T17:06:07.052367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's take a look at the length of characters:\nlen(\"\".join(sorted(set(text.lower()))))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:06:07.056115Z","iopub.execute_input":"2023-09-13T17:06:07.056528Z","iopub.status.idle":"2023-09-13T17:06:07.104505Z","shell.execute_reply.started":"2023-09-13T17:06:07.056489Z","shell.execute_reply":"2023-09-13T17:06:07.091363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>2. Text Preprocessing</div></b>","metadata":{}},{"cell_type":"markdown","source":"This section focuses on preprocessing the raw text data. We create a TextVectorization layer that tokenizes the text at the character level and converts all characters to lowercase for consistency. The layer is adapted to the text data, allowing us to efficiently encode the text into numerical sequences. We also check the shape of the encoded text to understand its dimensions.","metadata":{}},{"cell_type":"code","source":"# Let's create a TextVectorization layer for character-level tokenization:\ntext_vec_layer = tf.keras.layers.TextVectorization(\n    split=\"character\",standardize=\"lower\")","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:06:07.106493Z","iopub.execute_input":"2023-09-13T17:06:07.109783Z","iopub.status.idle":"2023-09-13T17:06:10.296634Z","shell.execute_reply.started":"2023-09-13T17:06:07.109747Z","shell.execute_reply":"2023-09-13T17:06:10.295720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's adapt the TextVectorization layer to the text data:\ntext_vec_layer.adapt([text])","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:06:10.297859Z","iopub.execute_input":"2023-09-13T17:06:10.298219Z","iopub.status.idle":"2023-09-13T17:06:10.871227Z","shell.execute_reply.started":"2023-09-13T17:06:10.298185Z","shell.execute_reply":"2023-09-13T17:06:10.870203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the shape of the encoded text:\ntext_vec_layer([text]).shape","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:06:10.873024Z","iopub.execute_input":"2023-09-13T17:06:10.873455Z","iopub.status.idle":"2023-09-13T17:06:11.440342Z","shell.execute_reply.started":"2023-09-13T17:06:10.873420Z","shell.execute_reply":"2023-09-13T17:06:11.439304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's preprocess the text:\nencoded = text_vec_layer([text])[0]\nencoded","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:06:11.443643Z","iopub.execute_input":"2023-09-13T17:06:11.443929Z","iopub.status.idle":"2023-09-13T17:06:11.959291Z","shell.execute_reply.started":"2023-09-13T17:06:11.443904Z","shell.execute_reply":"2023-09-13T17:06:11.958289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The TextVectorization layer assigns 0 for padding tokens and 1 for unknown characters. Since we currently don't need these tokens, we subtract 2 from the character IDs and calculate both the count of distinct characters and the total character count.","metadata":{}},{"cell_type":"code","source":"# Letâ€™s subtract 2 from the character IDs and compute the number of distinct characters and the total number of characters:\nencoded -= 2\nn_tokens = text_vec_layer.vocabulary_size()-2 \nn_tokens","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:06:11.963801Z","iopub.execute_input":"2023-09-13T17:06:11.964120Z","iopub.status.idle":"2023-09-13T17:06:11.976780Z","shell.execute_reply.started":"2023-09-13T17:06:11.964094Z","shell.execute_reply":"2023-09-13T17:06:11.975441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's take a look at the length of the dataset:\ndataset_size = len(encoded)\ndataset_size","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:06:11.978595Z","iopub.execute_input":"2023-09-13T17:06:11.979180Z","iopub.status.idle":"2023-09-13T17:06:11.986988Z","shell.execute_reply.started":"2023-09-13T17:06:11.979144Z","shell.execute_reply":"2023-09-13T17:06:11.986037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>3. Dataset Preparation </div></b>","metadata":{}},{"cell_type":"markdown","source":"Here, we define a function called to_dataset that converts the encoded text sequences into a dataset suitable for training. This function segments the text into overlapping sequences of a specified length and organizes them into batches. Optionally, it shuffles the dataset to enhance randomness during training. An example usage of the to_dataset function is provided to illustrate its functionality.","metadata":{}},{"cell_type":"code","source":"# Let's create a function to convert text sequences into a dataset\ndef to_dataset(sequence,length,shuffle=False,seed=None,batch_size=32):\n    ds = tf.data.Dataset.from_tensor_slices(sequence)\n    ds = ds.window(length + 1, shift=1,drop_remainder=True)\n    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n    if shuffle:\n        ds = ds.shuffle(100_000, seed=seed)\n    ds = ds.batch(batch_size)\n    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:06:11.988253Z","iopub.execute_input":"2023-09-13T17:06:11.989175Z","iopub.status.idle":"2023-09-13T17:06:11.997326Z","shell.execute_reply.started":"2023-09-13T17:06:11.989130Z","shell.execute_reply":"2023-09-13T17:06:11.996296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's get an example and pass it to the function:\nlist(to_dataset(text_vec_layer([\"I like\"])[0], length=5))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:06:11.998476Z","iopub.execute_input":"2023-09-13T17:06:11.998964Z","iopub.status.idle":"2023-09-13T17:06:12.110340Z","shell.execute_reply.started":"2023-09-13T17:06:11.998933Z","shell.execute_reply":"2023-09-13T17:06:12.109312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create the training, validation and test datasets.","metadata":{}},{"cell_type":"code","source":"length = 100\ntf.random.set_seed(42)\n# The training dataset:\ntrain_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True,seed=42)\n# The validation dataset:\nvalid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n# Test dataset:\ntest_set = to_dataset(encoded[1_060_000:], length=length)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:06:12.112291Z","iopub.execute_input":"2023-09-13T17:06:12.113450Z","iopub.status.idle":"2023-09-13T17:06:12.204192Z","shell.execute_reply.started":"2023-09-13T17:06:12.113413Z","shell.execute_reply":"2023-09-13T17:06:12.203216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>4. Model Definition and Training </div></b>","metadata":{}},{"cell_type":"markdown","source":"In this part of the code, we define the architecture of a neural network model for text generation. The model consists of an Embedding layer for representing tokens, a GRU (Gated Recurrent Unit) layer for sequence modeling, and a Dense layer with a softmax activation for predicting the next character. We compile the model using the sparse categorical cross-entropy loss and the Nadam optimizer. We also incorporate a ModelCheckpoint callback to save the best model weights during training. The model is then trained on the prepared datasets using the fit method.","metadata":{}},{"cell_type":"code","source":"# Let's define the model architecture:\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n    tf.keras.layers.GRU(128,return_sequences=True),\n    tf.keras.layers.Dense(n_tokens,activation=\"softmax\")\n])\n# Let's compile the model:\nmodel.compile(loss=\"sparse_categorical_crossentropy\", \n              optimizer=\"nadam\", metrics=[\"accuracy\"])\n\n#  Let's train the model and save the best checkpoints:\nmodel_ckpt = tf.keras.callbacks.ModelCheckpoint(\"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)\n\n# Let's train the model:\nhistory = model.fit( train_set, validation_data=valid_set, epochs=3,callbacks=[model_ckpt])","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:06:12.206361Z","iopub.execute_input":"2023-09-13T17:06:12.207053Z","iopub.status.idle":"2023-09-13T17:27:36.126723Z","shell.execute_reply.started":"2023-09-13T17:06:12.207018Z","shell.execute_reply":"2023-09-13T17:27:36.125672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's add the text preprocessing layer:\nshakespeare_model = tf.keras.Sequential([\n    text_vec_layer,\n    tf.keras.layers.Lambda(lambda X: X - 2),\n    model\n])","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:27:36.128740Z","iopub.execute_input":"2023-09-13T17:27:36.129686Z","iopub.status.idle":"2023-09-13T17:27:36.394561Z","shell.execute_reply.started":"2023-09-13T17:27:36.129657Z","shell.execute_reply":"2023-09-13T17:27:36.393289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>5. Text Generation </div></b>","metadata":{}},{"cell_type":"markdown","source":"This section defines a higher-level model for text generation, combining the TextVectorization layer, character-level adjustment, and the previously trained text generation model. This model can be used to generate text based on an initial input.","metadata":{}},{"cell_type":"code","source":"# Let's generate text using the trained model:\ny_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]\ny_pred = tf.argmax(y_proba)\ntext_vec_layer.get_vocabulary()[y_pred + 2]","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:27:36.395913Z","iopub.execute_input":"2023-09-13T17:27:36.396719Z","iopub.status.idle":"2023-09-13T17:27:36.903349Z","shell.execute_reply.started":"2023-09-13T17:27:36.396684Z","shell.execute_reply":"2023-09-13T17:27:36.902356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>6. Text Generation Functions</div></b>","metadata":{}},{"cell_type":"markdown","source":"Here, we define two important functions for text generation. The next_char function predicts the next character in a sequence given a context and a temperature parameter that controls the randomness of predictions. The extend_text function extends a given text with additional characters by iteratively predicting the next character based on the context. Example usages of these functions are provided to demonstrate how to generate text with different temperatures.","metadata":{}},{"cell_type":"code","source":"# How to use the tf.random.categorical() method:\nlog_probas = tf.math.log([[0.6, 0.3, 0.1]])\ntf.random.set_seed(42)\ntf.random.categorical(log_probas, num_samples=10)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:27:36.905349Z","iopub.execute_input":"2023-09-13T17:27:36.906354Z","iopub.status.idle":"2023-09-13T17:27:36.935999Z","shell.execute_reply.started":"2023-09-13T17:27:36.906320Z","shell.execute_reply":"2023-09-13T17:27:36.934902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's create a function to generate the next character based on input text:\ndef next_char(text, temperature=1):\n    y_proba = shakespeare_model.predict([text])[0, -1:]\n    rescaled_logits = tf.math.log(y_proba) / temperature\n    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n    return text_vec_layer.get_vocabulary()[char_id + 2]","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:27:36.937584Z","iopub.execute_input":"2023-09-13T17:27:36.938171Z","iopub.status.idle":"2023-09-13T17:27:36.945437Z","shell.execute_reply.started":"2023-09-13T17:27:36.938137Z","shell.execute_reply":"2023-09-13T17:27:36.944460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's create a function to extend a given text with additional characters:\ndef extend_text(text, n_chars=50, temperature=1):\n    for _ in range(n_chars):\n        text += next_char(text, temperature)\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:27:36.946876Z","iopub.execute_input":"2023-09-13T17:27:36.947512Z","iopub.status.idle":"2023-09-13T17:27:36.954793Z","shell.execute_reply.started":"2023-09-13T17:27:36.947480Z","shell.execute_reply":"2023-09-13T17:27:36.953790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's generate a text with a low temperature:\ntf.random.set_seed(42)\nprint(extend_text(\"I like\", temperature=0.01))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:27:36.957476Z","iopub.execute_input":"2023-09-13T17:27:36.958067Z","iopub.status.idle":"2023-09-13T17:27:41.052891Z","shell.execute_reply.started":"2023-09-13T17:27:36.958035Z","shell.execute_reply":"2023-09-13T17:27:41.051265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's create a higher temperature text:\nprint(extend_text(\"I like\", temperature=1))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:27:41.054144Z","iopub.execute_input":"2023-09-13T17:27:41.054522Z","iopub.status.idle":"2023-09-13T17:27:44.826496Z","shell.execute_reply.started":"2023-09-13T17:27:41.054480Z","shell.execute_reply":"2023-09-13T17:27:44.825554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In conclusion, the provided code demonstrates a comprehensive pipeline for text data preprocessing and text generation using TensorFlow and Keras. \n\nThanks for reading. If you enjoy this notebook, don't forget upvote. ","metadata":{}},{"cell_type":"markdown","source":"## Reference:\n\n- [Hands-on Machine Learning with Scikit-Learn and TensorFlow](https://github.com/ageron/handson-ml3/blob/main/16_nlp_with_rnns_and_attention.ipynb)","metadata":{}}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}